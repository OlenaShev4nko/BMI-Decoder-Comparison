{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QQGr3xvhyEdR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ebd0d89-bf61-454a-db6c-1ab69262a0ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhJaUsNFx2hc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8970a573-6471-4c8d-836d-bb7a727c7384"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mne\n",
            "  Downloading mne-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from mne) (3.1.4)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.10/dist-packages (from mne) (0.4)\n",
            "Requirement already satisfied: matplotlib>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from mne) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from mne) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mne) (24.1)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.10/dist-packages (from mne) (1.8.2)\n",
            "Requirement already satisfied: scipy>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from mne) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mne) (4.66.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne) (4.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->mne) (2.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.5.0->mne) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2024.7.4)\n",
            "Downloading mne-1.7.1-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mne\n",
            "Successfully installed mne-1.7.1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import codecs, json\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import time\n",
        "import math\n",
        "!pip install mne\n",
        "from mne.decoding import CSP\n",
        "import mne\n",
        "\n",
        "import sys\n",
        "sys.path.append('drive/MyDrive/BmiResearch')\n",
        "from constants import constants"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "constants.SIGNAL_PROCESSING"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSbpPqQMruVm",
        "outputId": "4ee6178f-530c-46b5-88cb-a0d51796e1b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['SLF', 'ASR', 'NOSP']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "constants.BASE_DATASET_PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3rL9fTzFhDui",
        "outputId": "5dd8d402-e2dc-44ed-a75e-eaefecda9850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'drive/MyDrive/BmiResearch/data/datasets/preprocessed/a_walk_in_the_park/baseline_chunked_data'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reconstruct_timeseries(chunks, size, overlap):\n",
        "  stride = size - overlap\n",
        "  final_series_list = []\n",
        "  for chunk in chunks:\n",
        "    final_series_list.append(chunk.T[0:stride])\n",
        "  result = np.concatenate(final_series_list, axis = 0)\n",
        "  return result\n"
      ],
      "metadata": {
        "id": "yeZJcdLNYY-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "output_path:  drive/MyDrive/BmiResearch/data/datasets/preprocessed/a_walk_in_the_park/SLF_CSP\n",
        "\n",
        "actual_base_dataset_path =  drive/MyDrive/BmiResearch/data/datasets/preprocessed/a_walk_in_the_park/SLF\n",
        "\n",
        "sub-006, sub-009, sub-011, sub-013, 'sub-016', 'sub-017'\n",
        "\n",
        "at fit: ValueError: array must not contain infs or NaNs\n",
        "\n"
      ],
      "metadata": {
        "id": "QVxmjHj4PuWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sp in ['SLF', 'ASR', 'NOSP']:\n",
        "  output_path = f'{constants.PREPROCESSED_DATASET_PATH}/{sp}_CSP'\n",
        "  print('output_path: ', output_path)\n",
        "\n",
        "  if sp == 'NOSP':\n",
        "    actual_base_dataset_path = constants.BASE_DATASET_PATH\n",
        "  else:\n",
        "    actual_base_dataset_path = f'{constants.PREPROCESSED_DATASET_PATH}/{sp}'\n",
        "  print(\"actual_base_dataset_path = \", actual_base_dataset_path)\n",
        "\n",
        "  for subject in sorted(os.listdir(actual_base_dataset_path)):\n",
        "    print(subject)\n",
        "    experiment_settings = dict()\n",
        "    experiment_settings['general_params'] = {'low_filter':constants.low_filter,\n",
        "                                            'high_filter':constants.high_filter,\n",
        "                                            'frequency':constants.freq,\n",
        "                                            'minutes_for_test':constants.minutes_for_test,\n",
        "                                            'window_size':constants.window_size,\n",
        "                                            'overlap':constants.overlap,\n",
        "                                            'EEG_CHANNELS':constants.EEG_CHANNELS}\n",
        "    experiment_settings['feature_extraction'] = 'CSP'\n",
        "    experiment_settings['DateTime'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    experiment_settings['BASE_DATASET_PATH'] = constants.BASE_DATASET_PATH\n",
        "    experiment_settings['OUTPUT_PATH'] = output_path\n",
        "\n",
        "    output_path_subject = (f'{output_path}/{subject}')\n",
        "    os.makedirs(output_path_subject)\n",
        "\n",
        "    experiment_settings[subject] = dict()\n",
        "\n",
        "    csp = CSP(n_components=25, reg=None, log=True, norm_trace=False)\n",
        "\n",
        "    # process train\n",
        "    chanks_train = codecs.open(f'{actual_base_dataset_path}/{subject}/X_train_chunks.json', 'r', encoding='utf-8').read()\n",
        "    chanks_train = json.loads(chanks_train)\n",
        "    chanks_train = np.array(chanks_train)\n",
        "    print(\"chanks_train = \", chanks_train.shape)\n",
        "\n",
        "    final_y_train_list = codecs.open(f'{constants.BASE_DATASET_PATH}/{subject}/y_train_chunks.json', 'r', encoding='utf-8').read()\n",
        "    final_y_train_list = json.loads(final_y_train_list)\n",
        "    final_y_train_list = np.array(final_y_train_list)\n",
        "\n",
        "\n",
        "    csp.fit(chanks_train, final_y_train_list)\n",
        "    csp_filters = csp.filters_\n",
        "\n",
        "    chanks_train_csp = np.asarray([np.dot(csp_filters, epoch) for epoch in chanks_train])\n",
        "\n",
        "    train_reconstracted = reconstruct_timeseries(chanks_train_csp, 100, 80)\n",
        "    train_reconstracted = train_reconstracted.tolist()\n",
        "    json.dump(train_reconstracted, codecs.open(f'{output_path_subject}/X_fit.json', 'w', encoding='utf-8'),\n",
        "    separators=(',', ':'),\n",
        "    sort_keys=True,\n",
        "    indent=4)\n",
        "\n",
        "    chanks_train_csp = chanks_train_csp.tolist()\n",
        "    json.dump(chanks_train_csp, codecs.open(f'{output_path_subject}/X_train_chunks.json', 'w', encoding='utf-8'),\n",
        "      separators=(',', ':'),\n",
        "      sort_keys=True,\n",
        "      indent=4)\n",
        "\n",
        "    # process test\n",
        "    chanks_test = codecs.open(f'{actual_base_dataset_path}/{subject}/X_test_chunks.json', 'r', encoding='utf-8').read()\n",
        "    chanks_test = json.loads(chanks_test)\n",
        "    chanks_test = np.array(chanks_test)\n",
        "\n",
        "    starttime = time.perf_counter()\n",
        "    chanks_test_csp = np.asarray([np.dot(csp_filters, epoch) for epoch in chanks_test])\n",
        "    proc_end = time.perf_counter() - starttime\n",
        "    proc_1ch_s = round(proc_end / chanks_test_csp.shape[0], 5)\n",
        "    experiment_settings[subject]['proc_1_test_ch_s'] = proc_1ch_s\n",
        "    experiment_settings[subject]['proc_test_s'] = round(proc_end, 5)\n",
        "    experiment_settings[subject]['len_test'] = chanks_test.shape[0]\n",
        "    experiment_settings[subject]['output_path_subject'] = output_path_subject\n",
        "\n",
        "    chanks_test_csp = chanks_test_csp.tolist()\n",
        "    json.dump(chanks_test_csp, codecs.open(f'{output_path_subject}/X_test_chunks.json', 'w', encoding='utf-8'),\n",
        "      separators=(',', ':'),\n",
        "      sort_keys=True,\n",
        "      indent=4)\n",
        "\n",
        "    json.dump(experiment_settings, codecs.open(f'{output_path_subject}/experiment_settings.json', 'w', encoding='utf-8'),\n",
        "      separators=(',', ':'),\n",
        "      sort_keys=True,\n",
        "      indent=4)\n"
      ],
      "metadata": {
        "id": "YqT_h1U8yEan"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}