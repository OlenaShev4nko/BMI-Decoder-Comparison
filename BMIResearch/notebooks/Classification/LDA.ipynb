{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcvO7eR1nxFV",
        "outputId": "70c87472-f81d-4dc6-fa08-7c90aa08a5db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8GK_qJ2n1yh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import codecs, json\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import time\n",
        "import math\n",
        "import joblib\n",
        "\n",
        "import sys\n",
        "sys.path.append('drive/MyDrive/BmiResearch')\n",
        "from constants import constants\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "import tracemalloc\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3tBAExZEjiH"
      },
      "outputs": [],
      "source": [
        "class_names_dict = {'RightTO': 0, 'RightHS': 1, 'LeftTO': 2, 'LeftHS': 3}\n",
        "class_names_dict_1 = {0: 'RightTO',  1: 'RightHS',  2: 'LeftTO',  3: 'LeftHS'}\n",
        "\n",
        "def calculate_weighted_metrics(y_test, y_pred):\n",
        "    \"\"\"'weighted':\n",
        "    Calculate metrics for each label, and find their average weighted by support\n",
        "    (the number of true instances for each label). This alters ‘macro’ to account for\n",
        "    label imbalance; it can result in an F-score that is not between\n",
        "    precision and recall.\"\"\"\n",
        "    ACC = accuracy_score(y_test, y_pred)\n",
        "    PPV = precision_score(y_test, y_pred, average='weighted')\n",
        "    TPR = recall_score(y_test, y_pred, average='weighted')\n",
        "    F1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    return ACC, PPV, TPR, F1\n",
        "\n",
        "\n",
        "def calculate_weighted_metrics_per_class(y_test, y_pred):\n",
        "    ACC = accuracy_score(y_test, y_pred)\n",
        "    PPV = precision_score(y_test, y_pred, average=None)\n",
        "    TPR = recall_score(y_test, y_pred, average=None)\n",
        "    F1 = f1_score(y_test, y_pred, average=None)\n",
        "    PPV = [round(el, 3) for el in PPV]\n",
        "    TPR = [round(el, 3) for el in TPR]\n",
        "    F1 = [round(el, 3) for el in F1]\n",
        "    return ACC, PPV, TPR, F1\n",
        "\n",
        "def apply_standard_scaling(data_chanks_list_train, one_scaler):\n",
        "    print('[apply_standard_scaling]')\n",
        "    print(\"data_chanks_list_train shape = \", data_chanks_list_train[0].shape)\n",
        "    final_train_set = []\n",
        "    for chank_df in tqdm(data_chanks_list_train):\n",
        "        final_train_set.append(one_scaler.transform(chank_df.T).T)\n",
        "    return np.array(final_train_set)\n",
        "\n",
        "def flat_aray(chanks):\n",
        "    new_chanks = [el.flatten().copy() for el in chanks]\n",
        "    return np.array(new_chanks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fv51OQXkn11f"
      },
      "outputs": [],
      "source": [
        "for sp in constants.SIGNAL_PROCESSING:\n",
        "  for fe in constants.FEATURE_EXTRACTION:\n",
        "    output_path = f'{constants.MODELS}/lda/{sp}_{fe}'\n",
        "    print('output_path: ', output_path)\n",
        "\n",
        "    if (sp == 'NOSP') & (fe == 'NOFE'):\n",
        "      dataset_path = constants.BASE_DATASET_PATH\n",
        "    elif (sp != 'NOSP') & (fe == 'NOFE'):\n",
        "      dataset_path = f'{constants.PREPROCESSED_DATASET_PATH}/{sp}'\n",
        "    else:\n",
        "      dataset_path = f'{constants.PREPROCESSED_DATASET_PATH}/{sp}_{fe}'\n",
        "\n",
        "    print('dataset_path: ', dataset_path)\n",
        "\n",
        "    for subject in sorted(os.listdir(dataset_path)):\n",
        "      print(subject)\n",
        "      experiment_settings = dict()\n",
        "      experiment_settings['general_params'] = {'low_filter':constants.low_filter,\n",
        "                                              'high_filter':constants.high_filter,\n",
        "                                              'frequency':constants.freq,\n",
        "                                              'minutes_for_test':constants.minutes_for_test,\n",
        "                                              'window_size':constants.window_size,\n",
        "                                              'overlap':constants.overlap,\n",
        "                                              'EEG_CHANNELS':constants.EEG_CHANNELS}\n",
        "      experiment_settings['subject'] = subject\n",
        "      experiment_settings['signal_processing'] = sp\n",
        "      experiment_settings['feature_extraction'] = fe\n",
        "      experiment_settings['classification'] = 'LDA'\n",
        "      experiment_settings['DateTime'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "      experiment_settings['dataset_path'] = dataset_path\n",
        "      output_path_subject = (f'{output_path}/{subject}')\n",
        "      experiment_settings['OUTPUT_PATH'] = output_path_subject\n",
        "      os.makedirs(output_path_subject)\n",
        "\n",
        "      # fit\n",
        "      fit_df = codecs.open(f'{dataset_path}/{subject}/X_fit.json', 'r', encoding='utf-8').read()\n",
        "      fit_df = json.loads(fit_df)\n",
        "      fit_df = np.array(fit_df)\n",
        "\n",
        "      one_scaler = StandardScaler()\n",
        "      one_scaler.fit(fit_df)\n",
        "\n",
        "      # train\n",
        "      chanks_train = codecs.open(f'{dataset_path}/{subject}/X_train_chunks.json', 'r', encoding='utf-8').read()\n",
        "      chanks_train = json.loads(chanks_train)\n",
        "      chanks_train = np.array(chanks_train)\n",
        "\n",
        "      # test\n",
        "      chanks_test = codecs.open(f'{dataset_path}/{subject}/X_test_chunks.json', 'r', encoding='utf-8').read()\n",
        "      chanks_test = json.loads(chanks_test)\n",
        "      chanks_test = np.array(chanks_test)\n",
        "\n",
        "      # y train\n",
        "      final_y_train_list = codecs.open(f'{constants.BASE_DATASET_PATH}/{subject}/y_train_chunks.json', 'r', encoding='utf-8').read()\n",
        "      final_y_train_list = json.loads(final_y_train_list)\n",
        "      final_y_train_list = np.array(final_y_train_list)\n",
        "\n",
        "      # y test\n",
        "      final_y_test_list = codecs.open(f'{constants.BASE_DATASET_PATH}/{subject}/y_test_chunks.json', 'r', encoding='utf-8').read()\n",
        "      final_y_test_list = json.loads(final_y_test_list)\n",
        "      final_y_test_list = np.array(final_y_test_list)\n",
        "\n",
        "      lda_train_set = apply_standard_scaling(chanks_train, one_scaler)\n",
        "\n",
        "      starttime = time.perf_counter()\n",
        "      lda_test_set = apply_standard_scaling(chanks_test, one_scaler)\n",
        "      duration_standard_scaling_s = (time.perf_counter() - starttime)\n",
        "      scale_1ch_s = round(duration_standard_scaling_s / chanks_test.shape[0], 10)\n",
        "      experiment_settings['scale_1ch_s'] = scale_1ch_s\n",
        "\n",
        "      lda_train_set = flat_aray(lda_train_set)\n",
        "      lda_test_set = flat_aray(lda_test_set)\n",
        "      X_train1, X_val, y_train1, y_val = train_test_split(lda_train_set, final_y_train_list,\n",
        "                                                                                  test_size=0.2,\n",
        "                                                                                  random_state=42)\n",
        "\n",
        "      starttime = time.perf_counter()\n",
        "      tracemalloc.start()\n",
        "      model = LinearDiscriminantAnalysis()\n",
        "      # define grid\n",
        "      grid = dict()\n",
        "      grid['solver'] = ['lsqr']\n",
        "      grid['shrinkage'] = ['auto', 0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6,\n",
        "                           0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n",
        "\n",
        "      # # define search\n",
        "      grid = GridSearchCV(model, grid, scoring='accuracy', cv=5, verbose=3)\n",
        "      # perform the search\n",
        "      results = grid.fit(X_val, y_val)\n",
        "      bp = results.best_params_\n",
        "      experiment_settings['best_params'] = bp\n",
        "      # experiment_settings['best_params'] = {'shrinkage': 0.55, 'solver': 'lsqr'}\n",
        "      print('Mean Accuracy: %.3f' % results.best_score_)\n",
        "      print('Config: %s' % results.best_params_)\n",
        "      model = LinearDiscriminantAnalysis(solver=bp['solver'], shrinkage=bp['shrinkage'])\n",
        "      model.fit(lda_train_set, final_y_train_list)\n",
        "      joblib.dump(model, f'{output_path_subject}/model_lda_{sp}_{fe}.pkl')\n",
        "      current, peak_train_MB = tracemalloc.get_traced_memory()\n",
        "      duration_train_min = round((time.perf_counter() - starttime) / 60, 3)\n",
        "      print(\n",
        "          f\"Final current memory usage, MB [{current / (1024 * 1024):0.2f}]~peak memory usage, MB [{peak_train_MB / (1024 * 1024):0.2f}]~time [{duration_train_min}] minutes, \")\n",
        "\n",
        "      tracemalloc.reset_peak()\n",
        "      tracemalloc.clear_traces()\n",
        "      tracemalloc.stop()\n",
        "\n",
        "      starttime = time.perf_counter()\n",
        "      tracemalloc.start()\n",
        "\n",
        "      predictions = model.predict(lda_test_set)\n",
        "      labels = unique_labels(final_y_test_list, predictions)\n",
        "      labels = [class_names_dict_1[el] for el in labels]\n",
        "      ACC_w, PPV_w, TPR_w, F1_w = calculate_weighted_metrics(final_y_test_list, predictions)\n",
        "      ACC, PPV, TPR, F1 = calculate_weighted_metrics_per_class(final_y_test_list, predictions)\n",
        "\n",
        "      current, peak_predict = tracemalloc.get_traced_memory()\n",
        "      sec_predict = round((time.perf_counter() - starttime), 3)\n",
        "      tracemalloc.reset_peak()\n",
        "      tracemalloc.clear_traces()\n",
        "      tracemalloc.stop()\n",
        "\n",
        "      proc_1ch_s = round(sec_predict / chanks_test.shape[0], 5)\n",
        "      experiment_settings['pred_1_ch_s'] = proc_1ch_s\n",
        "      experiment_settings['len_test'] = chanks_test.shape[0]\n",
        "\n",
        "      experiment_settings['y_test'] = list(final_y_test_list[:])\n",
        "      experiment_settings['prediction'] = list(predictions[:])\n",
        "      # print('Y test:', list(final_y_test_list[:]))\n",
        "      # print('Prediction:', list(predictions[:]))\n",
        "      experiment_settings['labels'] = labels\n",
        "      cm = confusion_matrix(final_y_test_list, predictions, normalize='true')\n",
        "      experiment_settings['confusion_matrix'] = experiment_settings['confusion_matrix'] = cm.tolist()\n",
        "      print(cm)\n",
        "      print(\"ACC_w, PPV_w, TPR_w, F1_w = \", ACC_w, PPV_w, TPR_w, F1_w)\n",
        "      print(\"ACC, PPV, TPR, F1 = \", ACC, PPV, TPR, F1)\n",
        "\n",
        "      experiment_settings['peak_predict_MB'] = round(peak_predict / (1024 * 1024), 2)\n",
        "      experiment_settings['sec_predict'] = sec_predict\n",
        "      experiment_settings['accuracy_score'] = round(ACC_w, 3)\n",
        "      experiment_settings['precision_score'] = round(PPV_w, 3)\n",
        "      experiment_settings['recall_score'] = round(TPR_w, 3)\n",
        "      experiment_settings['f1_score'] = round(F1_w, 3)\n",
        "      print(\"round(F1_w, 3) = \", round(F1_w, 3))\n",
        "\n",
        "      for i, val in enumerate(labels):\n",
        "          experiment_settings[f'{val}_precision_score'] = f'{round(PPV[i], 3)}'\n",
        "          experiment_settings[f'{val}_recall_score'] = f'{round(TPR[i], 3)}'\n",
        "          experiment_settings[f'{val}_f1_score'] = f'{round(F1[i], 3)}'\n",
        "\n",
        "      json.dump(experiment_settings, codecs.open(f'{output_path_subject}/experiment_results.json', 'w', encoding='utf-8'),\n",
        "      separators=(',', ':'),\n",
        "      sort_keys=True,\n",
        "      indent=4, default=str)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNHF2wWMn13v"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}